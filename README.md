# InterpretationFragility
Code for implementation of [Interpretation of Nueral Network is Fragile.](https://arxiv.org/pdf/1710.10547.pdf).

**Please cite the following work if you use this benchmark or the provided tools or implementations:**
```
@inproceedings{ghorbani2019interpretation,
  title={Interpretation of neural networks is fragile},
  author={Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3681--3688},
  year={2019}
}
```
## Authors

* **Amirata Ghorbani** - [Website](http://web.stanford.edu/~amiratag)
* **Abubakar Abid** - [Website](https://abidlabs.github.io/)
* **James Zou** - [Website](https://sites.google.com/site/jamesyzou/)


## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## The large scale results of attack methods against four famous feature-attribution methods
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/SaliencyMethodsComparison.png)

## Examples of targeted attack for semantically meaningful change in feature-importance
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/SemanticChange.png)

## Attack examples on Deep Taylor Decomposition
![alt text](https://github.com/amiratag/InterpretationFragility/blob/master/pictures/DTD_examples.png)

